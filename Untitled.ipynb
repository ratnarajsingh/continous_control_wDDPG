{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report : Continous Control\n",
    "\n",
    "### Index\n",
    "1. Problem statement\n",
    "2. Policy gradient and Actor-Critic Approach\n",
    "3. DDPG Algorithm\n",
    "4. Implementation details\n",
    "5. Experiment and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement\n",
    "\n",
    "##### Environment :\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. I other words, this consists of a moving location or 'orb' , and a controllable agent\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "##### Agent :\n",
    "Its a moveable, double-jointed arm with 4 degrees of freedom or in other words, 4 actions.\n",
    "\n",
    "##### Challenge : \n",
    "Design and develop an agent that should be able to follow the orb or target location while its moving. In mathematical terms. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='reacher_multi/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Gradient and Actor-Critic Approach\n",
    "\n",
    "#### Policy Gradient\n",
    "It offers a solution to the problem of having continous action space, which are difficult to solve with popular methods like _sarsa_ or DQN. *Value-Based Methods* like DQN or TD(0) obtain an optimal policy $\\pi_*$ by trying to estimate the optimal action-value function, *Policy-Based Methods* directly learn the optimal policy. This simplification allows these type of methods to handle handle either discrete or continuous actions. They also have to advantage of combining Monte-Carlo returns with TD returns (Actor-Critic for example)\n",
    "Policy based methods offer advatages like :\n",
    "* Better conversion properties. This comes from the fact that value based methods tend to oscillate as it chooses different actions while training\n",
    "* Works better in high dimensional space having continous action space, in contrast to DQN\n",
    "* Can learn stochastic policies as well, DQN cannot\n",
    "\n",
    "#### Actor Critic Approach\n",
    "This approaches uses two function approximator namely an Actor and a Critic.\n",
    "* Actor learns a stochastic policy by using a Monte-Carlo approach. It decides the probability of action(s) to take.  \n",
    "* Critic learns the value function. It is used to tell whether the actions taken by the Actor are good or not.\n",
    "Proess is as follows\n",
    "\n",
    "1. Observe state $s$ from environment \n",
    "2. Use Actor to get action distribution $\\pi(a|s;\\theta_\\pi)$. Stochasticially select one action stochastically and feed back to the environment.  \n",
    "3. Observe next state $s'$ and reward $r$.  \n",
    "4. Use the tuple $(s, a, r, s')$ for the TD estimate $y=r + \\gamma V(s'; \\theta_v)$\n",
    "5. Use critic loss as $L=(y - V(s;\\theta_v)^2$ for training the Critic network to minimize it \n",
    "6. Calculate the advantage $A(s,a) = r + \\gamma V(s'; \\theta_v) - V(s; \\theta_v)$ and use it to traing the Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DDPG\n",
    "\n",
    "\n",
    "**Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2016]** is an off policy algorithm that combines the above explained Actor-Critic approach with the well known DQN. Below is a brief description of  \n",
    "- The actor function $\\mu(s;\\theta_\\mu)$ gives the current policy. It maps states to continuous deterministic actions\n",
    "- The critic $Q(s,a;\\theta_q)$ on used to calculate action values and is learned using the Bellman equation\n",
    "- Use of replay buffer - tuples of $(s, a, r, s')$ are stored and then batches are sampled to train the networks. This reduced the issue of network being trained from correlated tuples,which arises from sequentially exploring the environment\n",
    "- Use of *target networks*, similar to DQN, allowed stable learning. A regular but softly (explained below) updated copy of the network is used as taget, for both Actor and Critic. They are used to calculate target values\n",
    "\n",
    "Soft update allows only a fraction of the current/local network to change target network weights. This allows for continous target value improvments, unlike DQN where target network is update after every few thousand iterations with full replication:\n",
    "\n",
    "$ \\theta' \\leftarrow \\tau \\theta + (1-\\tau)\\theta'$ with $\\tau \\ll 1$\n",
    "\n",
    "Batch normalization is used while training to learn the importance of entire state space. Exploration in continous space is handled ny adding a noise distribution process $N$ an exploration policy $\\mu'$ is constructed:\n",
    "\n",
    "$\\mu'(s_t) = \\mu(s_t;\\theta_{\\mu,t})+N$\n",
    "\n",
    "The DDPG pseudocode:\n",
    "\n",
    "\n",
    "<img src='img/Pseudocode.png' height='200'/>\n",
    "\n",
    "\n",
    "\n",
    "### 4. Implementation details\n",
    "\n",
    "For details of the implementation please refer to the class definition files and DDPG_Control notebook. Brief description of each of the files is given below\n",
    "* noise.py : Contains the class definition of noise function. It follows OU noise distribution without decay.\n",
    "* model.py : Contains the definitions of Actor, Critic and Replay buffer class. All these classes are instantiated in the DDPG class, which implements the DDPG algorithm explained above\n",
    "* ddpg.py : Contains the class definition of DDPG agent. It instantiates a pair , each of Actor and Critic classes, to be used as local and target network. The algorithm details is primarily defined in _act_ and _learn_ function of the class\n",
    "* DDPG_Control.ipynb : Contains details of overall implementation, from setting up the environment to episodic learning of solving the reacher environment using DDPG agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experiment and Results\n",
    "\n",
    "It was noted that batch size is an important factor in learning. Steadily increasing batch size reduced episodes taken to learn.\n",
    "This is a score graph for a batch size of 2048\n",
    "\n",
    "<img src='img/score_graph.png' height='100'>\n",
    "\n",
    "\n",
    "And this one is for a batch size of 512\n",
    "\n",
    "\n",
    "Other parameters chosen are\n",
    "Network parameters : A typical size of 128 and 256 is chosen as layer size.\n",
    "Tau = 0.01 -> I chose a slightly higher update rate for faster learning. This led to slightly turbulent episodes, but it did the job\n",
    "max_steps = 5000 ; approximate maximum time steps to get a total reward of 30\n",
    "buffer_size = 10000000\n",
    "\n",
    "Parameters below have assigned their typical values \n",
    "actor learning rate = 0.0001\n",
    "critic learning rate = 0.001\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
